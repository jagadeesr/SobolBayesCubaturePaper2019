% This is a general template file for the LaTeX package SVJour3
% for Springer journals. Original by Springer Heidelberg, 2010/09/16
%
% Use it as the basis for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%
\documentclass[graybox,footinfo]{svmult}

\smartqed
% \usepackage{mathptmx}       % selects Times Roman as basic font
\usepackage{helvet}         % selects Helvetica as sans-serif font
\usepackage{courier}        % selects Courier as typewriter font
\usepackage{type1cm}        % activate if the above 3 fonts are
% not available on your system
\usepackage{graphicx}       % standard LaTeX graphics tool
% when including figure files
%\usepackage{newtxtext}
%\usepackage{newtxmath}
%\usepackage{txfonts}

% \documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
%\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%

% \vec is not showing arruw, so redefine
% \AtBeginDocument{\renewcommand{\vec}[1]{\overrightarrow{#1}}}

% This approach is better than the above
% remove the definition of \vec by svjour3
\let\vec\relax
% restore the original definition in fontmath.ltx
\DeclareMathAccent{\vec}{\mathord}{letters}{"7E}

% \RequirePackage{amsmath}
% \RequirePackage{fix-cm}

% \smartqed  % flush right qed marks, e.g. at end of proof
%
% \usepackage{graphicx}
\usepackage{amssymb}
% \usepackage{amsthm}

% \usepackage{natbib}
\usepackage{xspace}
% \usepackage{mathrsfs}  
% \usepackage{hhline}
% \usepackage{siunitx}
% \usepackage{bbm}


%
% insert here the call for the packages your document requires
%\usepackage{mathptmx}      % use Times fonts if available on your TeX system
%\usepackage{latexsym}
% etc.
%
% Jag's 
% \usepackage{tabu}
% \usepackage{cancel}
\usepackage{algorithm}
% \usepackage{algorithmicx}
\usepackage{algpseudocode}
%\usepackage[style=base]{caption}
%\usepackage[caption=false]{subfig}
%\usepackage{subcaption}
\usepackage{booktabs, mathtools}
%\usepackage[dvipsnames]{xcolor}
% \usepackage{url}
% \usepackage[titletoc,title]{appendix}
%\usepackage{refcheck}  % points out unused labels, refs ...

%\captionsetup{compatibility=false}

\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}%

\DeclareMathOperator{\Order}{{\mathcal O}}

% please place your own definitions here and don't use \def but
% \newcommand{}{}
%\newtheorem{prop}{Proposition}
%\newtheorem{defn}{Definition}
\providecommand{\HickernellFJ}{Hickernell\xspace}
\newcommand{\bm}[1]{\boldsymbol{#1}}
\newcommand{\mSigma}{\mathsf{\Sigma}}
%\newcommand{\mB}{\mathsf{B}}
\newcommand{\smallocite}[1]{{\small\ocite{#1}}}
\newcommand{\dif}[1]{\text{d}{#1}}
% \newcommand{\D}[1]{\text{d}{#1}}
\newcommand{\trace}[1]{\textup{trace}{#1}}

\newcommand{\naturals}{\mathbb{N}}
\newcommand{\natzero}{\mathbb{N}_0}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\integers}{\mathbb{Z}}
\newcommand{\posIntegers}{\mathbb{Z}_{> 0}}
\newcommand{\complex}{\mathbb{C}}
\newcommand{\hilbert}{\mathbb{H}}
\newcommand{\Ex}{\mathbb{E}}
\newcommand{\bbone}{\mathbbm{1}}

\newcommand{\cf}{\mathcal{F}}
\newcommand{\cl}{\mathcal{L}}
\newcommand{\bbcl}{\bar{\boldsymbol{\cl}}}
\newcommand{\hbcl}{\hat{\boldsymbol{\cl}}}
\newcommand{\cx}{\mathcal{X}}
\newcommand{\tcx}{\widetilde{\cx}}
\newcommand{\rC}{\mathring{C}}
\newcommand{\ry}{\mathring{y}}
\newcommand{\rlambda}{\mathring{\lambda}}
\newcommand{\calU}{\mathcal{U}}

\newcommand{\valpha}{{\bm{\alpha}}}
\newcommand{\vbeta}{{\bm{\beta}}}
\newcommand{\vDelta}{{\boldsymbol{\Delta}}}
\newcommand{\veta}{{\bm{\eta}}}
\newcommand{\vlambda}{{\bm{\lambda}}}
\newcommand{\vphi}{{\bm{\phi}}}
\newcommand{\vpsi}{{\bm{\psi}}}
\newcommand{\vtheta}{{\bm{\theta}}}
\newcommand{\vzeta}{{\bm{\zeta}}}
\newcommand{\vthetaMLE}{\bm{\theta}_{\MLE}}
\newcommand{\hvtheta}{\hat{\vtheta}}
\newcommand{\va}{\bm{a}}
\newcommand{\vA}{\bm{A}}
\newcommand{\vb}{\bm{b}}
\newcommand{\tvb}{\widetilde{\vb}}
\newcommand{\vc}{\bm{c}}
\newcommand{\vC}{\bm{C}}
\newcommand{\tvc}{\tilde{\bm{c}}}
\newcommand{\vg}{\bm{g}}
\newcommand{\vh}{\bm{h}}
\newcommand{\vf}{\bm{f}}
\newcommand{\vk}{\bm{k}}
\newcommand{\vl}{\bm{l}}
\newcommand{\vm}{\bm{m}}
\newcommand{\vs}{\bm{s}}
\newcommand{\vt}{\bm{t}}
\newcommand{\vv}{\bm{v}}
\newcommand{\vV}{\bm{V}}
\newcommand{\vw}{\bm{w}}
\newcommand{\vW}{\bm{W}}
\newcommand{\vx}{\bm{x}}
\newcommand{\dx}{\dif{{x}}}
\newcommand{\dt}{\dif{{t}}}
\newcommand{\dvx}{\dif{\bm{x}}}
\newcommand{\dvs}{\dif{\bm{s}}}
\newcommand{\dvt}{\dif{\bm{t}}}
\newcommand{\vrho}{\bm{\rho}}
\newcommand{\hy}{\hat{y}}
\newcommand{\vy}{\bm{y}}
\newcommand{\vY}{\bm{Y}}
\newcommand{\hvy}{\hat{\vy}}
\newcommand{\vz}{\bm{z}}
\newcommand{\vZ}{\bm{Z}}
\newcommand{\dvz}{\dif{\bm{z}}}
\newcommand{\tf}{\tilde{f}}
\newcommand{\vPsi}{\boldsymbol{\Psi}}
\newcommand{\vi}{\boldsymbol{i}}
\newcommand{\ai}{\overrightarrow{\imath}}
\newcommand{\aj}{\overrightarrow{\jmath}}
\newcommand{\az}{\overrightarrow{z}}
\newcommand{\ak}{\overrightarrow{k}}
\newcommand{\ax}{\overrightarrow{x}}
\newcommand{\abk}{\overrightarrow{\vk}}
\newcommand{\abx}{\overrightarrow{\vx}}

\newcommand{\fC}{\mathfrak{C}}
\newcommand{\hfC}{\mathring{\fC}}
\newcommand{\hbfC}{\mathring{\boldsymbol{\fC}}}
\newcommand{\fc}{\mathfrak{c}}
\newcommand{\bfc}{\boldsymbol{\mathfrak{c}}}

\newcommand{\tvv}{\tilde{\vv}}
\newcommand{\tvz}{\tilde{\vz}}

\newcommand{\vCvtheta}{{C_\vtheta}}
\newcommand{\hc}{\widehat{c}}

\newcommand{\hatvy}{\hat{\bm{y}}}
\newcommand{\haty}{\hat{y}}
\newcommand{\tvy}{\widetilde{\bm{y}}}
\newcommand{\ty}{\widetilde{y}}
\newcommand{\vzero}{\bm{0}}
\newcommand{\vone}{\bm{1}}
\newcommand{\tvone}{\tilde{\bm{1}}}
\newcommand{\mA}{\mathsf{A}}
\newcommand{\mC}{\mathsf{C}}
\newcommand{\mP}{\mathsf{P}}
\newcommand{\mK}{\mathsf{K}}
\newcommand{\rmC}{\mathring{\mathsf{C}}}
\newcommand{\mCtheta}{{\mathsf{C}_{\vtheta}}}
\newcommand{\mCthetaInv}{{\mathsf{C}^{-1}_{\vtheta}}}
%\newcommand{\mCthetaMLE}{{\mathsf{C}_{\vthetaMLE}}}
%\newcommand{\mCthetaInvMLE}{{\mathsf{C}^{-1}_{\vthetaMLE}}}
\newcommand{\mCInv}{\mathsf{C}^{-1}}
\newcommand{\cov}{{\textup{cov}}}
\newcommand{\var}{{\textup{var}}}
\newcommand{\opt}{{\textup{opt}}}


\newcommand{\tmC}{\widetilde{\mathsf{C}}}
\newcommand{\tlambda}{\tilde{\lambda}}

\newcommand{\mL}{\mathsf{L}}

\newcommand{\mLambda}{\mathsf{\Lambda}}
\newcommand{\mLambdaInv}{\mathsf{\Lambda}^{-1}}

\newcommand{\mG}{\mathsf{G}}
\newcommand{\mH}{\mathsf{H}}
\newcommand{\mV}{\mathsf{V}}
\newcommand{\mW}{\mathsf{W}}

\newcommand{\calN}{\mathcal{N}}
\newcommand{\me}{\mathrm{e}}

\newcommand{\tvrho}{\widetilde{\vrho}}
\newcommand{\heta}{\hat{\eta}}
\newcommand{\hmu}{\widehat{\mu}}
\newcommand{\hsigma}{\widehat{\sigma}}
\newcommand{\hnu}{\hat{\nu}}
\newcommand{\rhoCond}{\mathring{\vrho}}

\newcommand{\MVN}{\textup{MVN}}
\newcommand{\MLE}{\textup{EB}}
\newcommand{\full}{\textup{full}}
\newcommand{\GCV}{\textup{GCV}}


\newcommand{\wal}{\textup{wal}}
\newcommand{\CI}{\textup{CI}}
\newcommand{\NICE}{\textup{smth}}
\newcommand{\PEAKY}{\textup{pky}}
\newcommand{\NOISE}{\textup{noise}}
\newcommand{\TRUE}{\textup{true}}

%\newcommand{\errtol}{\text{tol}}
\newcommand{\errtol}{\varepsilon}
\newcommand{\errn}{\text{err}_{n}}
\newcommand{\diag}{\text{diag}}
\newcommand{\err}{\textup{err}}
\newcommand{\code}[1]{\texttt{#1}}

\def\abs#1{\ensuremath{\left \lvert #1 \right \rvert}}
\newcommand{\norm}[2][{}]{\ensuremath{\left \lVert #2 \right \rVert}_{#1}}
\newcommand{\ip}[3][{}]{\ensuremath{\left \langle #2, #3 \right \rangle_{#1}}}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\newenvironment{nalign}{
    \begin{equation}
    \begin{aligned}
}{
    \end{aligned}
    \end{equation}
    \ignorespacesafterend
}

\providecommand{\argmin}{\operatorname*{argmin}}
\providecommand{\argmax}{\operatorname*{argmax}}
\newcommand\figref{Figure~\ref}
\newcommand\secref{Section~\ref}

\graphicspath{{.}{./figures/grey/}{D:/Mega/MyWriteupBackup/Sep_2ndweek_1/}}
%\graphicspath{{./figures/}}

%
% Insert the name of "your journal" with
% \journalname{Automatic Bayesian Cubature}
%

\newcommand{\FJHNote}[1]{{\textcolor{blue}{FJH: #1}}}
\newcommand{\JRNote}[1]{{\textcolor{green}{JR: #1}}}


\allowdisplaybreaks
\begin{document}
\setlength\abovedisplayskip{1pt}
\setlength{\belowdisplayskip}{1pt}

\title{Fast Automatic Bayesian Cubature Using Sobol' Sampling
%\thanks{}
}
% Grants or other notes about the article that should go on the front
% page should be placed within the \thanks{} command in the title
% (and the %-sign in front of \thanks{} should be deleted)
%
% General acknowledgments should be placed at the end of the article.

%\subtitle{Do you have a subtitle?\\ If so, write it here}

%\titlerunning{Short form of title}        % if too long for running head

\author{R. Jagadeeswaran         \and
        Fred J. Hickernell %etc.
}

%\authorrunning{Short form of author list} % if too long for running head

\institute{R. Jagadeeswaran \at
              Department of Applied Mathematics, \\
              Illinois Institute of Technology \\
              10 W. 32nd St., Room 220,
              Chicago IL 60616\\
              \email{jrathin1@iit.edu}           %  \\
%             \emph{Present address:} of F. Author  %  if needed
           \and
           Fred J. Hickernell \at
           Center for Interdisciplinary Scientific Computation and \\
           Department of Applied Mathematics \\
           Illinois Institute of Technology \\
           10 W. 32nd St., Room 220, 
           Chicago IL 60616
           \\
           \email{hickernell@iit.edu} 
}

\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor

\maketitle

\abstract{
	Automatic cubatures approximate integrals to user-specified error tolerances.  For high dimensional problems, it is difficult to adaptively change the sampling pattern, but one can automatically determine the sample size, $n$, given a reasonable, fixed sampling pattern. This approach is pursued in \cite{RatHic19a}, where a Bayesian perspective is used to construct a credible interval for the integral, and the computation is terminated when the half-widths of the interval is no greater than the required error tolerance.  In \cite{RatHic19a} integration lattice sampling is employed and the computations are expedited by the use of a fast Fourier transform because the covariance kernels for the Gaussian process prior on the integrand are chosen to be shift-invariant.  In this chapter, we extend our fast automatic Bayesian cubature to digital net sampling via digital shift-invariant covariance kernels and fast Walsh transforms.
	Our algorithm is implemented in the Guaranteed Automatic Integration Library (GAIL).
}





\section{Introduction}
\label{intro}

Cubature, or numerical multivariate integration, is the problem of inferring a numerical value for a definite integral, 
\begin{equation}
\label{eqn:defn_mu}
\mu:= \mu(f) := \Ex[f(\boldsymbol{X})] = \int_{[0,1]^d} f(\vx)\, \dvx, 
\end{equation}
when no closed-form analytic expression exists. Typically, values of $f$ are accessible through a black-box function routine. Our goal is to construct a cubature, $\hmu = \hmu(f)$, depending only on integrand values at the nodes $\{\vx_i\}_{i=1}^n$, and determine the $n$ which satisfies the error criterion
\begin{equation}
\label{eqn:err_crit} 
\abs{\mu - \hmu} \leq \errtol
\end{equation}
with high probability. This article extends the fast Bayesian cubature ideas presented in \cite{RatHic19a} to digital sequences \cite{DicPil10a}.


Cubature is a key component of many problems in scientific computing, finance \cite{Gla03}, statistical modeling, imaging \cite{Keller2013}, uncertainty quantification, and machine learning \cite{Goodfellow-et-al-2016}. 
%\JRNote{add references}
The original form of the integral may require a suitable variable transformation to become \eqref{eqn:defn_mu}. This process is addressed in \cite{BecHae92b, Sid08a, Sid93, Lau96a, CriEtal07}. 

Following the Bayesian numerics approach of \cite{Dia88a}, \cite{OHa91a}, Rasmussen and Ghahramani~\cite{RasGha03a}, \cite{BriEtal18a}, and others, we assume that our integrand is an instance of a Gaussian process, $\mathcal{GP}(m,s^2 C_\vtheta)$, and construct a probabilistic error bound for $\mu$ via a Bayesian credible interval.  The integrand is sampled until the credible interval becomes small enough to satisfy \eqref{eqn:err_crit} with high probability.  

Our approach to fast Bayesian cubature \cite{RatHic19a} emphasizes 
\begin{itemize}
    \item Choosing covariance kernels, $C_\vtheta:[0,1]^d \times [0,1]^d \to \reals$, for which the symmetric, positive definite Gram matrices, 
    \begin{equation} \label{eq:Gram}
        \mC_\vtheta = \left(  C_\vtheta(\vx_i,\vx_j)  \right)_{i,j=1}^n,
    \end{equation}
    have an eigenvalue-eigenvector decomposition of the form $\mC_\vtheta = \mV \mLambda_\vtheta \mV^H/n$, where
    \begin{subequations} \label{eq:fastcompAssump}
	\begin{align}
	\label{eq:fastcompAssumpA}
	& \mV \text{ may be identified analytically}, \\
	\label{eq:fastcompAssumpB}
	& \text{the first row and column of $\mV$ are } \vone, \\
	\label{eq:fastcompAssumpC}
	& \text{ Computing $\mV^H \vb$ requires only $\Order(n \log n)$ operations } \forall \vb, \\
	\label{addAssump}
    & \int_{[0,1]^d} C_\vtheta(\vt,\vx) \, \D \vt = 1 \qquad \forall \vx \in [0,1]^d,
	\end{align}
\end{subequations}
and
\item The hyperparameters of the Gaussian process, $m$, $s$, and $\vtheta$ are tuned to increase the likelihood that $f$ is a typical integrand and not an outlier.
\end{itemize}
We call the transformation $\vb \mapsto \mV^H \vb$ a \emph{fast Bayesian transform}. Our earlier work, \cite{RatHic19a}, which laid the foundation for our approach to fast Bayesian cubature, focused on lattice nodes and shift-invariant kernels. In that context the computation of $\mV^H \vb$ is a one-dimensional fast Fourier transform.  This chapter focuses on digital sequences, such as Sobol' sequences \cite{Sob67}, and covariance kernels that are digital shift-invariant.  In this case the computation of $\mV^H \vb$ is a fast Walsh-Hadamard transform. 


\iffalse 
We call the transformation $\vb \mapsto \mV^H \vb$ a \emph{fast Bayesian transform} and $C_\vtheta$ a \emph{fast Bayesian transform kernel} for the matching nodes $\{\vx_i\}_{i=1}^\infty$.  The covariance kernels used in practice also may satisfy an additional assumption:
\fi

The next section summarizes the key formulae from \cite{RatHic19a}.  Section \ref{sec:sobol_walsh} extends fast Bayesian cubature to digital nets and digital-shift invariant kernels defined in terms of Walsh functions.  Section \ref{sec:NumExp} presents numerical experiments that illustrate these ideas.  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Bayesian Cubature}
\label{sec:BC} 

% \subsection{Bayesian Posterior Error} \label{sec:BayesPostErr}

\FJHNote{Fred fix this section}

We assume the integrand, $f$, is an instance of a real-valued stochastic Gaussian process, i.e., $f \sim \mathcal{GP}(m,s^2 C_\vtheta)$.  Specifically, $f$ is a random function with constant mean $m$ and covariance kernel $s^2C_\vtheta$, where $s$ is a positive scale factor, and $C_\vtheta: [0,1]^d \times [0,1]^d \to \mathbb{R} $ is a symmetric, positive-definite kernel parameterized by $\vtheta$.  The parameter $\vtheta$ may affect the shape or smoothness of $C_\vtheta$. As shown in \cite[Give eq numbers]{RatHic19a}, under assumptions \eqref{eq:fastcompAssump} a credible interval for the integral is given in terms of the nodes, $\{\vx_i\}_{i=1}^n$, the function data, $\vy = \bigl (f (\vx_i) \bigr)_{i=1}^n$, the scale parameter, $s$, and the kernel, $C_\vtheta$,  by 
\begin{subequations} \label{eqn_prob_confidence_interval}
	\begin{gather}
	\mathbb{P}_f \left[
	|\mu-\hmu| \leq \err_{\CI}
	\right] = 99\%, \\
	\intertext{where}
	\label{muhatGCV-FB-MLE-Simple}
	\hmu = \frac 1n \sum_{i=1}^n y_i, \\
	\label{eqn:errcifast}
	\err_{\CI} = 2.58 s \sqrt{1 - n/\lambda_{\vtheta,1} }.
	\end{gather}
\end{subequations}
Moreover, $\lambda_{\vtheta,1}$ represents the first eigenvalue of the Gram matrix, $\mC_\vtheta$, defined in \eqref{eq:Gram}.  All eigenvalues can be computed %(by the which is computed) 
by taking the fast Bayesian transform of the first column of the Gram matrix, denoted $\vC_{\vtheta,1}$ \cite[Equation number]{RatHic19a}:
\begin{equation} 
	\vlambda_\vtheta
	= \begin{pmatrix}
		\lambda_{\vtheta,1} \\ \vdots \\ \lambda _{\vtheta,n}
	\end{pmatrix} = \diag(\mLambda) = \mLambda \vone 
	= \mV^H \vC_{\vtheta,1}.
	\label{eqn:fast_transform_to_eigvalues}
\end{equation}

%\subsection{Resolving Hyperparameters} \label{sec:hyperparameters}
In \eqref{eqn_prob_confidence_interval} the hyperparameter $m$ has already been tuned, but $s$ and $\vtheta$ have not yet been.  %We use the function data % JR: Commented this
This formula for the credible interval depends essentially on $s$ and $\vtheta$.  Resolving these dependencies should also involve the integrand data,  $\vy$, since larger integrand data should tend to imply a larger credible interval. The data should also be used to identify the covariance kernel for which the integrand is a typical draw from the assumed Gaussian process. This is the motivation behind resolving the hyperparameters from function data. 
\JRNote{Need to rephrase this paragraph}

In \cite{RatHic19a} we introduced three methods for resolving the hyperparameters:  empirical Bayes (EB), full Bayes (full), and generalized cross-validation (GCV).  Under assumptions \eqref{eq:fastcompAssump}, we have the estimates for the hyperparameters \cite[Theorem 2]{RatHic19a}:
		\begin{align*}
		\nonumber 
		\widetilde{\vy} &:= \mV^H \vy, \\
%		\nonumber
%		m_\MLE &=  m_{\full} = m_{\GCV} =  \frac{\widetilde{y}_1}{n} = \frac 1n \sum_{i=1}^n y_i,
%		\\
		\nonumber
		s^2_\MLE 
		& =
		\frac{1}{n^2} 
		\sum_{i=2}^n \frac{\abs{\widetilde{y}_i}^2}{\lambda_{\vtheta,i}},  \\
		\nonumber 
		s^2_{\textup{GCV}} & =  \frac 1{n} \sum_{i=2}^n \frac{\abs{\widetilde{y}_i}^2}{\lambda_{\vtheta,i}^2}  \left [ \sum_{i=1}^n \frac{1}{\lambda_{\vtheta,i}} \right]^{-1}, \\
		\nonumber
		\widehat{\sigma}^2_{\textup{full}} &= \frac{1}{n(n-1)} \sum_{i=2}^n \frac{\abs{\widetilde{y}_i}^2}{\lambda_{\vtheta,i}}  \left(\frac{\lambda_{\vtheta,1}}{n}  - 1  \right), 
		\end{align*}
	\begin{subequations}
		\label{eqn:fastTheta}
		\begin{align}
		\label{eqn:fastThetaMLE}
		\vtheta_\MLE
		&= 
		\argmin_{\vtheta}
		\left[
		\log\left(
		\sum_{i=2}^n \frac{\abs{\widetilde{y}_i}^2}{\lambda_{\vtheta,i}}
		\right) 
		+ \frac{1}{n}\sum_{i=1}^n \log(\lambda_{\vtheta,i})
		\right],\\
		\label{eqn:fastThetaGCV} 
		\vtheta_{\GCV} 
		&= \argmin_\vtheta \left[ \log \left ( \sum_{i=2}^n \frac{\abs{\widetilde{y}_i}^2}{\lambda_{\vtheta,i}^2} 
		\right)  
		-2\log\left( \sum_{i=1}^n \frac{1}{\lambda_{\vtheta,i}} \right)
		\right].
		\end{align}
	\end{subequations}
\begin{subequations}
	\label{errSimple}
	\begin{align}
		\label{eqn:err_MLEGCV}
		\err_{\CI,\mathsf{x}} = \err_{\mathsf{x}} & = 2.58 s_{\mathsf{x}} \sqrt{1 - \frac{n}{\lambda_{\vtheta,1}} }, \qquad \mathsf{x} \in \{\MLE, \GCV\},  \\ 
		\err_{\CI,\textup{full}} 
		& = t_{n-1,0.995} \hsigma_{\textup{full}} > \err_\MLE. \label{FJH:eq:errFull}
	\end{align}
\end{subequations}
In the formulas for the credible interval half-widths, $\vtheta$ is assumed to take on the values $\vtheta_{\MLE}$ or $\vtheta_{\GCV}$ as appropriate.

Our  Bayesian cubature algorithm increases the sample size until the width of the credible interval is small enough.  This is accomplished through successively doubling the sample size; these steps are detailed in Algorithm~\ref{algorithm1}.


\algnewcommand{\IIf}[1]{\State\algorithmicif\ #1\ \algorithmicthen\ }
\algnewcommand{\IElse}{\unskip\ \algorithmicelse\ }
\algnewcommand{\EndIIf}{\unskip\ \algorithmicend\ \algorithmicif}


\begin{algorithm}
	\caption{Automatic Bayesian Cubature}\label{algorithm1}
	\begin{algorithmic}[1]
		\Require a generator for the sequence
		$\vx_1, \vx_2, \ldots$; 
		a black-box function, $f$; 
		an absolute error tolerance,
		$\varepsilon>0$; the positive initial sample size, $n_0$;
		the maximum sample size $n_{\textup{max}}$
		
		\State $n \gets n_0, \; n' \gets 0, \; \err_\CI \gets \infty$
		
		\While{$\err_\CI > \varepsilon$ and $n \le n_{\textup{max}}$}
		\State Generate $\{ \vx_i\}_{i=n' + 1}^{n}$ and sample $\{f(\vx_i)\}_{i=n'+1}^{n}$
		\State Compute $\vtheta$ by \eqref{eqn:fastThetaMLE} or \eqref{eqn:fastThetaGCV}
		\State Compute $\err_\CI$  according to \eqref{eqn:err_MLEGCV} or \eqref{FJH:eq:errFull}
		
		\State	$n' \gets n, \; n \gets 2n'$
		
		\EndWhile
		
		\State Sample size to compute $\hmu$, $n \gets n'$
		\State Compute $\hmu$, the approximate integral, according to \eqref{muhatGCV-FB-MLE-Simple}
		\State \Return $\hmu, \; n$  and $\err_\CI$
	\end{algorithmic}
\end{algorithm}

We recognize that multiple applications of our credible intervals in one run of the algorithm is not strictly justified.  However, if our integrand comes from the middle of the sample space  and not the extremes, we expect our automatic Bayesian cubature to approximate the integral within the desired error tolerance with high probability. The examples in Section~\ref{sec:NumExp} support that expectation. 

The credible intervals used in our automatic algorithm are homogeneous with respect to the function data.  If they are valid for some integrand, $f$, they are also valid for the integrand $a f$ for any constant $a$.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Digital Nets and Walsh Kernels}
\label{sec:sobol_walsh}


The previous section does not mention which sequences of data sites and  kernels satisfy assumptions \eqref{eq:fastcompAssump}.  We demonstrated in \cite{RatHic19a} that rank-1 lattice points and shift-invariant kernels do so.  In this section, we give another example, namely 
digital sequences nets and Walsh kernels
The results of this chapter can be summarized as follows:

% \Section{Summary}


\begin{theorem}
	Any symmetric, positive definite, digital shift-invariant kernel of the form \eqref{eqn:walsh_kernel} scaled to satisfy \eqref{addAssump}, when matched with digital sequence data sites, satisfies assumptions \eqref{eq:fastcompAssump}.  The \emph{fast Walsh-Hadamard transform} performs the fast Bayesian transform,  $\vb \mapsto \mV^H \vb$ , in $\Order(n \log n)$ operations.
\end{theorem}
This section defines digital sequences, digital shift-invariant kernels, and the fast Walsh-Hadamard transform.

\subsection{Digital Sequences} \label{sec:sobol}
% Notations
% \ell - dimesnion
% i,j - point sequence index

The first example of a digital sequence was proposed by Sobol' \cite{Sob67}, and his Sobol' sequences are the most popular.  These were later generalized.  In this chapter we specialize to the case of base $2$, which includes Sobol' sequences.  Here is a definition of a digital sequence in base $2$.

\begin{definition} \label{def:digitalseq}
	For any non-negative integer $i = (\dots i_3 i_2 i_1)_2$, define $\ai = (i_1, i_2, \dots)^T$ as the $\infty \times 1$ vector $\ai$ of its binary digits. 
	For any point $z = {}_20.z_1 z_2 \dots \in [0, 1)$, define $\az = (z_1, z_2, \dots)^T$ as the $\infty \times 1$ vector of its binary digits. 
	Let $ \mathsf{G}_1, \dots , \mathsf{G}_d$ denote predetermined $\infty \times \infty$ generator matrices whose elements are zeros and ones. 
	A digital sequence in base $2$ is $\{\vz_1, \vz_2, \vz_3, \dots\}$, where each $\vz_i = ( z_{i1}, \dots , z_{id})^T \in [0, 1)^d$ is defined by
	\begin{align*}
	\az_{i+1,\ell} = \mathsf{G}_{\ell} \, \ai \pmod 2,  \quad \ell = 1, \dots, d, \quad i = 0, 1, \dots \;.
	\end{align*}
\end{definition}

It is common to index digital sequences starting with $0$, whereas our data sites and matrices in the earlier section are indexed starting with $1$.  To keep our notation consistent, we define $\vz_{i+1}$ in terms of $\ai$.

Digital sequences have a group structure under digitwise, element-by-element addition modulo the base, which we denote by $\oplus$ and which also corresponds to an exclusive-or. Here and in what follows we ignore the cases of measure zero for which the $\oplus$ operation leads to a binary represntation ending in an infinite string of ones.  The following lemma summarizes some important properties of digital sequences. 


\begin{lemma}
	\label{lemma:digital_net_prop}
	Let $\{\vz_i\}_{i=1}^{\infty}$ be a digital sequence in base $2$ as defined in Definition \ref{def:digitalseq}.  Choose any digital shift $\vDelta \in [0,1)^d$, and define $\{\vx_i\}_{i=1}^{\infty}$ by digitwise addition, $\vx_{i} = \vz_{i} \oplus \vDelta$.
	Then for all $i,j \in \naturals_0$,
	\begin{gather}
		\label{eqn:digital_net_symmetric_prop}
	\vx_{i+1} \oplus \vx_{i+1} = \boldsymbol{0}, \qquad 
	\vx_{i+1} \oplus \vx_{j+1} = \vx_{j+1} \oplus \vx_{i+1} \\
	\label{eqn:digital_shift_prop}
	\vx_{i+1} \oplus \vx_{j+1} = \vz_{i+1} \oplus \vz_{j+1} = \vz_{(i \oplus j) + 1 }.
	\end{gather}
	Therefore, $\{\vz_i\}_{i=1}^{\infty}$ is a group.  Moreover, $\{\vz_i\}_{i=1}^{2^m}$ is a subgroup for $m \in \natzero$.
\end{lemma}
The proof follows straightforwardly from Definition \ref{def:digitalseq} can be found in \cite{JagThesis19a}.

Digital sequence generators can be chosen by number theory, as in the case of Sobol' \cite{Sob67} and Niederreiter-Xing sequences \cite{NieXin01a} (see also \cite[Chapter 8]{DicPil10a} or they can be chosen by computer search \cite[Chapter 10]{DicPil10a}.  The original generator matrices may be scrambled using linear matrix scrambling \cite{Mat98}.


\subsection{Covariance Kernels Constructed via Walsh Functions} \label{sec:Walsh_kernels}

The digital shift invariant kernels required for fast Bayesian cubature using digital nets are constructed via Walsh functions, again specializing to base $2$. 
The one-dimensional Walsh functions in base $2$ are defined as
\begin{align}
    \label{eqn:walsh_func}
\textup{wal}_{k}(x) & := (-1)^{k_0 x_1 + k_1 x_2 + \cdots} = (-1)^{\langle \ak, \ax \rangle},  \qquad k \in \natzero, \ x \in [0,1), \\
\nonumber
\langle \ak, \ax \rangle & : = k_{0} x_{1} + k_{1} x_{2} + \cdots .
\end{align}
where again $\ak$ is a vector containing the binary digits of $k$, and $\ax$ is a vector containing the binary digits of $x$.  Note that by this definition, $\textup{wal}_{k}(x \oplus t) =  \textup{wal}_{k}(x) \textup{wal}_{k}(t)$.  

These Walsh functions can be used to construct a covariance kernel for univariate integrands as follows
\begin{align*}
    C_\vtheta (x,t) & = K_\vtheta(x \oplus t), \qquad K_\vtheta(x) : = 1 + \eta \omega_{r} (x), \qquad \vtheta = (r,\eta), \\
    \omega_r(x) & := \sum_{k=1}^\infty \frac{\textup{wal}_{k}(x) }{2^{2r \lfloor \log_2 k \rfloor}} \\
    \omega_{r} (x_\ell \oplus t_\ell) & = \sum_{k=1}^\infty \frac{\textup{wal}_{k}(x) \textup{wal}_{k}(t)  }{2^{2r \lfloor \log_2 k \rfloor}}.
\end{align*}
The symmetric, positive definite property of $ C_\vtheta (x,t)$ follows from its definition.  This kernel is digital shift invariant because 
\[
C_\vtheta (x \oplus \Delta ,t \oplus \Delta ) = K_\vtheta(x \oplus \Delta \oplus t \oplus \Delta) = K_\vtheta(x \oplus t) =  C_\vtheta (x,t).
\]
This follows because $\Delta \oplus \Delta = 0$.

An explicit expression is available for $\omega_{r}$ in the case of order $r=1$ \cite{Nuyens2013}:
\begin{equation}
\label{eqn:omega1}
\omega_1(x) 
% &= \prod_{l=1}^d \sum_{k=1}^\infty 
% \frac{\textup{wal}_{b,k}(x_l) }{b^{2 \lfloor \log_b k \rfloor}} 
= 1  - 6 \times 2^{\lfloor \log_2 x \rfloor -1 }.
\end{equation}
\figref{fig:walshkernel-dim1} shows $C_\vtheta (x,t)$ for order $r=1$ and various values of $\eta$ in the interval $[0,1)$. Smaller $\eta_\ell$ implies lesser variation in the amplitude of the kernel.  Unlike the shift-invariant kernels used with lattice nodes, low order Walsh kernels are discontinuous and are only piecewise constant. 

\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{"figures/walsh_kernel dim_1"}
	\caption[Walsh kernel]{Walsh kernel of order $r=1$ in dimension $d=1$. This figure can be reproduced using \code{plot\_walsh\_kernel.m}. %\JRNote{remove $r=1$ in the figure}
	}
	\label{fig:walshkernel-dim1}
\end{figure}

Covariance kernels for multivariate integrands defined on $[0,1)^d$ are constructed as tensor products:
\begin{align}
\label{eqn:digital_shift_in_kernel}
C_{\vtheta}(\vx, \vt) &= K_{\vtheta} (\vx \oplus \vt), \\
\label{eqn:walsh_kernel}
K_{\vtheta} (\vx) & =  
\prod_{\ell=1}^d  1 + \eta_\ell \omega_{r} (x_\ell), \quad \veta = (\eta_1, \cdots, \eta_d), \quad \vtheta = (r, \veta).
\end{align}
The parameter vector $\vtheta$ may now be of dimension $d+1$.

\subsection{Eigenvector-Eigenvalue Decomposition of the Gram Matrix}

For fast Bayesian cubature to succeed, the digital net data sites (\secref{sec:sobol}) and the covariance kernels (\secref{sec:Walsh_kernels}) must match in a way that conditions \eqref{eq:fastcompAssump} are satisfied.  To do this, we identify the eigenvectors of the Gram matrix defined in \eqref{eq:Gram}.  These are actually the Walsh-Hadamard matrices
\begin{align}
\nonumber
%\arraycolsep=1.4pt\def\arraystretch{0.9}
\mH^{(1)} &=
\begin{pmatrix}
1 & 1 \\ 1 & -1
\end{pmatrix}, \qquad
\mH^{(2)} = \mH^{(1)} \bigotimes \mH^{(1)} = 
\begin{pmatrix}
1 & 1 & 1 & 1 \\ 
1 & -1 & 1 & -1 \\
1 & 1 & -1 & -1 \\ 
1 & -1 & -1 & 1 \\
\end{pmatrix}, \cdots
\\
\label{eqn:hadamard_matrix}
\mH^{(m)} &= \mH^{(m-1)} \otimes \mH^{(1)} =
\begin{pmatrix}
\mH^{(m-1)} & \mH^{(m-1)} \\ \mH^{(m-1)} & -\mH^{(m-1)}
\end{pmatrix} 
= \mH^{(1)} \underbrace{\bigotimes \cdots \bigotimes}_{m \ \text{times}}  \mH^{(1)} 
% \\ \nonumber & = \Big((-1)^{\ai^T \aj)} \Big)_{i,j=0}^{2^m-1}.
\end{align}
where $\bigotimes$ is Kronecker product.
%, and $\ai^T \aj = i_0 j_0 + i_1 j_1 +\cdots$. 
Note that the Walsh-Hadamard matrices are symmetric.

\begin{lemma} \label{lemma:eig}
    Let $\left(\vx_i\right)_{i=1}^{2^m}$ be digitally shifted Sobol' nodes and let the covariance kernel take the form of \eqref{eqn:digital_shift_in_kernel}. Then, for $m = 1, 2, \ldots$ the Gram matrix, $
	\mCtheta^{(m)} = \bigl(C_\vtheta(\vx_i, \vx_j)\bigr)_{i,j=1}^{2^m} = \bigl(K(\vx_i \oplus \vx_j)\bigr)_{i,j=1}^{2^m}$ is a $2\times 2$ block-Toeplitz matrix and all the sub-blocks and their sub-sub-blocks, etc.\ are also $2\times 2$ block-Toeplitz. Moreover, $\mCtheta^{(m)} = \mH^{(m)} \mLambda^{(m)}\mH^{(m)}$, where $\mLambda^{(m)}$ is the diagonal matrix of eigenvalues of $\mCtheta^{(m)}$.
\end{lemma}

\begin{proof}
    First define the matrices that by Lemma \ref{lemma:digital_net_prop}, 
    \begin{align*}
    \mCtheta^{(m,k)} & = \bigl(K(\vx_i \oplus \vx_{j + k2^m} )\bigr)_{i,j=1}^{2^m} \\
    & = \bigl(K(\vz_{i\oplus j+ k 2^m + 1})\bigr)_{i,j=0}^{2^m-1}, \qquad m,k  = 0, 1, \ldots
    \end{align*}
    which means that $\mCtheta^{(m+1,k)}$, has the following block structure
    \begin{align} \nonumber
    \MoveEqLeft{\mCtheta^{(m+1,k)}} \\
    \nonumber
    & = 
    \begin{pmatrix} 
    \bigl(K(\vz_{i\oplus j+ k2^{m+1} + 1})\bigr)_{i,j=0}^{2^m-1} &
    \bigl(K(\vz_{i\oplus (j+2^m)+ k2^{m+1} +1})\bigr)_{i,j=0}^{2^m-1} \\
    \bigl(K(\vz_{(i + 2^m) \oplus j + k2^{m+1}+1})\bigr)_{i,j=0}^{2^m-1} &
    \bigl(K(\vz_{(i + 2^m) \oplus (j+2^m) + k2^{m+1} +1})\bigr)_{i,j=0}^{2^m-1}
    \end{pmatrix} \\
    & = \begin{pmatrix} \mCtheta^{(m,2k)} & \mCtheta^{(m,2k+1)} \\
    \mCtheta^{(m,2k+1)} & \mCtheta^{(m,2k)}
    \end{pmatrix} , \label{eq:cblock}
    \end{align}
    since for $i,j = 0, \ldots, 2^m-1$, it follows that 
    \begin{gather*}
        i\oplus (j+2^m) +1 = (i\oplus j) + 2^m +1 = (i + 2^m) \oplus j +1 ,\\
        (i + 2^m) \oplus (j+2^m) +1 = (i\oplus j) + (2^m \oplus 2^m) +1= (i\oplus j) +1.
    \end{gather*}
    
    The proof follows by induction.  Note that for the case $m=1$ and $k = 0, 1, \ldots$
    \begin{equation*}
        \mCtheta^{(1,k)} 
        =  \begin{pmatrix}
        K_\vtheta(\vz_{2k+1}) & K_\vtheta(\vz_{2k+2}) \\
        K_\vtheta(\vz_{2k+2}) & K_\vtheta(\vz_{2k+1}) 
        \end{pmatrix} ,
    \end{equation*}
   which has the desired Toeplitz property. Note also the eigenvectors of $\mCtheta^{(1,k)}$ are the columns of $\mH^{(1)}$ since   
   \begin{align*}
        \mCtheta^{(1,k)} \mH^{(1)} & =  \begin{pmatrix}
        K_\vtheta(\vz_{2k+1}) + K_\vtheta(\vz_{2k+2}) & K_\vtheta(\vz_{2k+1}) - K_\vtheta(\vz_{2k+2})\\
        K_\vtheta(\vz_{2k+2}{2k+2}) + K_\vtheta(\vz_{2k+1}) &  K_\vtheta(\vz_{2k+2}) - K_\vtheta(\vz_{2k+1}) 
        \end{pmatrix} \\
        & = \underbrace{\begin{pmatrix}
        K_\vtheta(\vz_{2k+1}) + K_\vtheta(\vz_{2k+2}) & 0\\
        0 &   K_\vtheta(\vz_{2k+1}) - K_\vtheta(\vz_{2k+2})
        \end{pmatrix}}_{\mLambda^{(1,k)}} \mH^{(1)}.
    \end{align*}

   Now assume that $\mCtheta^{(m,k)}$ is $2\times 2$ block-Toeplitz matrix with all its sub-blocks and sub-sub-blocks, etc.\ also $2\times 2$ block-Toeplitz for $k = 0, 1, \ldots$.  Then by \eqref{eq:cblock}, the same holds for $\mCtheta^{(m+1,k)}$ for $k = 0, 1, \ldots$.  In particular, the block Toepliz property of $\mCtheta^{(m,0)}$ holds.
   
    Moreover, the hypothesized eigenvectors of $\mCtheta^{(m+1,k)}$ can also be verified by direct calculation under the induction hypothesis:
     \begin{align*}
        \MoveEqLeft{\mCtheta^{(m+1,k)} \mH^{(m+1)}} \\
        & =  
        \begin{pmatrix} \mCtheta^{(m,2k)} & \mCtheta^{(m,2k+1)} \\
        \mCtheta^{(m,2k+1)} & \mCtheta^{(m,2k)}
        \end{pmatrix} 
        \begin{pmatrix}
        \mH^{(m)} &   \mH^{(m)} \\
         \mH^{(m)} & - \mH^{(m)}
        \end{pmatrix}\\
        & = 
    \begin{pmatrix}
        [\mCtheta^{(m,2k)} + \mCtheta^{(m,2k+1)}] \mH^{(m)} & [\mCtheta^{(m,2k)} - \mCtheta^{(m,2k+1)}] \mH^{(m)}\\
        [\mCtheta^{(m,2k+1)} + \mCtheta^{(m,2k)}] \mH^{(m)} &   [\mCtheta^{(m,2k+1)} - \mCtheta^{(m,2k)}] \mH^{(m)}
        \end{pmatrix} \\
        & = \underbrace{\begin{pmatrix}
        \mLambda_\vtheta^{(m,2k)} + \mLambda_\vtheta^{(m,2k+1)}  & \mathsf{0}\\
        \mathsf{0} &   \mLambda_\vtheta^{(m,2k)} - \mLambda_\vtheta^{(m,2k+1)}
        \end{pmatrix}}_{\mLambda_\vtheta^{(m+1,2k)}} 
        \begin{pmatrix}
        \mH^{(m)} & \mH^{(m)} \\ \mH^{(m)} & -\mH^{(m)}
        \end{pmatrix}.
    \end{align*}
Noting that $\mC^{(m+1)} = \mC^{(m+1,0)} $ completes the proof.
\end{proof}

Lemma \ref{lemma:eig} establishes that covariance kernels of the form \eqref{eqn:digital_shift_in_kernel} matched with shifted digital net data sites satisfy the fast Bayesian cubature assumptions
\begin{itemize}
    \item \eqref{eq:fastcompAssumpA}, since $\mV$ is the Walsh-Hadamard matrix,
    
    \item  \eqref{eq:fastcompAssumpB}, since the first column and row of the  Walsh-Hadamard matrix consists of all ones, and
    
    \item \eqref{addAssump}, since all Walsh functions but the zeroth integrate to zero.
\end{itemize}
What remains to be shown is how $\mV^H \vb = \mH \vb$ can be calculated in $\Order(n \log(n))$ operations for arbitrary $\vb$.

The computation of $\tvb =\mH \vb$ is done iteratively as follows and claim that it requires $m2^m$ operations for vectors $\vb$ of length $2^m$. For the case $m=0$, $\tvb = \mH^{(m)} \vb = \vb$, which requires no arithmetic operations.  Now, Let $\vb = (\vb_1^{T},  \vb_2^{T})$, where $\vb_1$ and $\vb_2$ are each of length $2^m$, and so $\vb$ is of length $2^{m+1}$.  Let $\tvb = \mH^{(m+1)} \vb$,  $\tvb_1 = \mH^{(m)} \vb_1$, and $\tvb_2 = \mH^{(m)} \vb_2$.  It follows from the definition of the Walsh-Hadamard matrix that 
\begin{align*}
\tvb &= \mH^{({m+1})} \vb = \begin{pmatrix}
\mH^{(m)} & \mH^{(m)} \\ \mH^{(m)} & - \mH^{(m)}
\end{pmatrix} 
\begin{pmatrix}
\vb_1 \\ \vb_2
\end{pmatrix} \qquad \text{by \eqref{eqn:hadamard_matrix}} \\
&= 
\begin{pmatrix}
\mH^{(m)} \vb_1 + \mH^{(m)}\vb_2 \\ 
\mH^{(m)} \vb_1 - \mH^{(m)} \vb_2
\end{pmatrix}
= 
\begin{pmatrix}
\tvb_1 + \tvb_2 \\ 
\tvb_1 - \tvb_2
\end{pmatrix}.
\end{align*}
Thus to compute $\tvb$ requires two matrix multiplications by $\mH^{(1)}$, at a cost of $m 2^m$ each, plus an addition and a subtraction of vectors of length $2^m$, for a total cost of $2 \times m 2^m + 2 \times 2^m = (m+1) 2^{m+1}$, which is exactly what is needed.


\FJHNote{Fred stopped here}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Jags: TODO
% Read and verify 1.3 Digital Nets
% Remove unnecessary packages
% f_Genz refer to old paper
%



\section{Numerical Experiments}

\label{sec:NumExp}

\JRNote{use uniformly randomly chosen $\varepsilon$ instead 4 fixed}

The fast Bayesian cubature algorithm using digital sequences and the digital shift invariant kernel in \eqref{eqn:digital_shift_in_kernel} with order $r=1$ has been coded as \code{cubBayesNet\_g}.  We illustrate our algorithm for three common examples, which were also treated in \cite{RatHic19a} via fast Bayesian cubature based on rank-1 lattices and shift invariant kernels and coded as \code{cubBayesLattice\_g}. The first example evaluates a multivariate Gaussian probability, the second example is Keister's function \cite{Kei96}, and the final example is pricing an Asian arithmetic mean option.  

The nodes used in \code{cubBayesNet\_g} were the randomly scrambled and shifted Sobol' points supplied by MATLAB's Sobol' sequence generator. Four hundred different error tolerances, $\varepsilon$, were randomly chosen such that $\log(\varepsilon)$ has a uniform distribution. 
For each integral example, each $\varepsilon$, and each stopping criteria---empirical Bayes, full Bayes, and generalized cross-validation---we ran \code{cubBayesNet\_g}.  For each run, the execution time is plotted against $\abs{\mu - \hmu}/\varepsilon$.  We expect $\abs{\mu - \hmu}/\varepsilon$ to be no greater than one, but hope that it is not too much smaller than one, which would indicate that the stopping criterion that is too conservative.

\subsection{Multivariate Gaussian Probability}

The integral if formulated as in \cite{RatHic19a} and following the variable transformation introduced by Alan Genz \cite{Gen92}. The simulation results are summarized in Figures \ref{fig:Sobol-mvn-guaranteed-MLE}, \ref{fig:Sobol-mvn-guaranteed-FB}, and \ref{fig:Sobol-mvn-guaranteed-GCV}.  In all cases, \code{cubBayesNet\_g} returns an approximation within the prescribed error tolerance. For  $\varepsilon=10^{-5}$ with the empirical Bayes stopping criterion, \code{cubBayesNet\_g} takes about 3 seconds as shown in \figref{fig:Sobol-mvn-guaranteed-MLE} whereas using a Mat\'ern kernel requires 30 seconds to obtain the same accuracy as shown in \cite{RatHic19a}. This highlights the speed-ups possible using fast Bayesian cubature.

The \code{cubBayesNet\_g} uses MATLAB's fast Walsh transform which is slower than MATLAB's fast Fourier transform, due to the latter's faster implementation in machine code.  We suspect that this is part of the reason that \code{cubBayesLattice\_g} is faster than \code{cubBayesNet\_g} for this example.  Also, \code{cubBayesLattice\_g} used on average $n \approx 16{,}000$ samples for $\varepsilon = 10^{-5}$, whereas \code{cubBayesNet\_g} used on average $n \approx 32{,}000$ samples.

Amongst the three stopping criteria, GCV achieved an acceptable approximation faster than others but it is less conservative. 
One can also observe from the figures that the credible intervals are narrower than in \figref{fig:Sobol-mvn-guaranteed-MLE}.
This shows that \code{cubBayesNet\_g} with $r=1$ kernel more accurately approximates the integrand.

\begin{figure}
\centering
%d=3 problem transformed into d=2
\includegraphics[width=0.95\linewidth]{"figures/Sobol/Sobol_MVN_guaranteed_time_MLE__d2_r1_2019-Sep-1"}
\caption[Sobol: MVN guaranteed: MLE]{Multivariate normal probability example with empirical Bayes stopping criterion.}
\label{fig:Sobol-mvn-guaranteed-MLE}
\end{figure}
\begin{figure}
\centering
%d=3 problem transformed into d=2
\includegraphics[width=0.95\linewidth]{"figures/Sobol/Sobol_MVN_guaranteed_time_full__d2_r1_2019-Sep-1"}
\caption[Sobol: MVN guaranteed: Full Bayes]{Multivariate normal probability example with the full-Bayes stopping criterion.}
\label{fig:Sobol-mvn-guaranteed-FB}
\end{figure}
\begin{figure}
\centering
%d=3 problem transformed into d=2
\includegraphics[width=0.95\linewidth]{"figures/Sobol/Sobol_MVN_guaranteed_time_GCV__d2_r1_2019-Sep-1"}
\caption[Sobol: MVN guaranteed: GCV]{Multivariate normal probability example with the GCV stopping criterion.}
\label{fig:Sobol-mvn-guaranteed-GCV}
\end{figure}






\subsection{Keister's Example}

This multidimensional integral function comes from \cite{Kei96} and is inspired by a physics application:
\begin{align}
\label{eqn:keister_integral}
\mu & =  \int_{\reals^d} \cos(\norm{ \vt}) \exp(-\norm{ \vt }^2) \, \dvt \\
\nonumber
%&  = 
%\int_{\reals^d} \cos(a\norm{ \vt}) \exp(-a^2\norm{ \vt }^2)  a^d \, \dvt \\
& = \int_{[0,1]^d} f_{\textup{Keister}}(\vx) \, \dvx,\\
\intertext{where }
\nonumber
f_\textup{Keister}(\vx) &= \pi^{d/2} \cos\left(\norm{ \Phi^{-1}(\vx)/2}\right)  ,
\end{align}
and $\Phi$ is the standard normal distribution.
%\begin{figure}
%	\captionsetup[subfigure]{labelformat=empty}
%	\begin{subfigure}[h]{0.48\linewidth}
%		\includegraphics[width=1.1\linewidth]{Keister_wholeR_1D}
%	\end{subfigure}
%	\begin{subfigure}[h]{0.48\linewidth}
%		\includegraphics[width=1.1\linewidth]{Keister_cube_1D}
%	\end{subfigure}
%	\caption{Keister function in d=2 and its transformed version in $[0,1]^2$ with various values of $a$.}
%	\label{fig:keister}
%\end{figure}
The true value of $\mu$ can be calculated iteratively in terms of a quadrature as follows:  
\begin{equation*}
\mu = \frac{2 \pi^{d/2} I_c(d)}{\Gamma(d/2)}, \quad d=1,2, \ldots
\end{equation*}
where $\Gamma$ denotes the gamma function, and
\JRNote{cite prev paper or the orig source instead}
\begin{align*}
I_c(1) &= \frac{\sqrt{\pi}}{2 \exp(1/4)}, 
\\
I_s(1) &= \int_{x=0}^\infty \exp(-\vx^T\vx)\sin(\vx) \, \dvx 
\\
& =  0.4244363835020225,
\\
I_c(2) &= \frac{1-I_s(1)}{2}, \qquad
I_s(2) = \frac{I_c(1)}{2}
\\
I_c(j) &= \frac{(j-2)I_c(j-2)-I_s(j-1)}{2},
\qquad j =3,4,\ldots
\\
I_s(j) &= \frac{(j-2)I_s(j-2)-I_c(j-1)}{2},
\qquad j =3,4,\ldots.
% ref: https://www.mathworks.com/help/matlab/ref/gamma.html
\end{align*}


Figures \ref{fig:Sobol-keister-guaranteed-MLE}, \ref{fig:Sobol-keister-guaranteed-FB} and \ref{fig:Sobol-keister-guaranteed-GCV} summarize the numerical tests for this case. We used  dimension $d=4$, and $r=1$.  No periodization transform was used as the integrand need not be periodic. 
% In this example, we use $r=1$ order kernel whereas \code{cubBayesLattice\_g} in \cite{RatHic19a}, smoother $r=2$ order kernel was used. 
In \cite{RatHic19a} \code{cubBayesLattice\_g} used a much smoother kernel that used here.
This necessitates \code{cubBayesNet\_g} to use more samples for integration.
As observed from the figures, the GCV stopping criterion, in Figure \ref{fig:Sobol-keister-guaranteed-GCV} achieved the results faster than the others but it is less conservative which is also the case with the multivariate Gaussian example.

\begin{figure}
\centering
\includegraphics[width=0.95\linewidth]{"figures/Sobol/Sobol_Keister_guaranteed_time_MLE__d4_r1_2019-Sep-1"}
\caption[Sobol: Keister guaranteed: MLE]{Keister example using the empirical Bayes stopping criterion.}
\label{fig:Sobol-keister-guaranteed-MLE}
\end{figure}
\begin{figure}
\centering
\includegraphics[width=0.95\linewidth]{"figures/Sobol/Sobol_Keister_guaranteed_time_full__d4_r1_2019-Sep-1"}
\caption[Sobol: Keister guaranteed: Full Bayes]{Keister example using the full-Bayes stopping criterion.}
\label{fig:Sobol-keister-guaranteed-FB}
\end{figure}
\begin{figure}
\centering
\includegraphics[width=0.95\linewidth]{"figures/Sobol/Sobol_Keister_guaranteed_time_GCV__d4_r1_2019-Sep-1"}
\caption[Sobol: Keister guaranteed: GCV]{Keister example using the GCV stopping criterion.}
\label{fig:Sobol-keister-guaranteed-GCV}
\end{figure}







\subsection{Option Pricing}

The price of financial derivatives can often be modeled by high dimensional integrals. If the underlying asset is described in terms of a discretized geometric Brownian motion, then the fair price of the option is:
\begin{equation*}
\mu = \int_{\reals^d} \text{payoff}(\vz) \frac{\exp(\frac 12 \vz^T\mSigma^{-1}\vz)}{\sqrt{(2\pi)^d \det(\mSigma)}} \, \dvz = \int_{[0,1]^d} f(\vx) \, \dvx,
\end{equation*} 
where {payoff($\cdot$)} defines the discounted payoff of the option,
\begin{align*}
\mSigma &= (T/d) \bigl(\min(j,k) \bigr)_{j,k=1}^d = \mL \mL^T,\\
f(\vx) &= \text{payoff} \left(\mL 
\begin{pmatrix}
\Phi^{-1}(x_1) \\ \vdots \\ \Phi^{-1}(x_d)
\end{pmatrix} \right).
\end{align*}
The Asian arithmetic mean call option has a payoff of the form
\begin{align*}
\text{payoff}(\vz) &= \max\left( \frac 1d  \sum_{j=1}^d S_j(\vz) - K, 0 \right) \me^{-r T}, \\
S_j(\vz) &= S_0 \exp\bigl((r-\sigma^2/2)jT/d + \sigma \sqrt{T/d} z_j \bigr).
\end{align*}
Here, $T$ denotes the time to maturity of the option, $d$ the number of time steps, $S_0$ the initial price of the stock, $r$ the interest rate, $\sigma$ the volatility, and $K$ the strike price.  

%\Subsection{Option Pricing}
The Figures \ref{fig:Sobol-optprice-guaranteed-MLE}, \ref{fig:Sobol-optprice-guaranteed-FB} and 
\ref{fig:Sobol-optprice-guaranteed-GCV} summarize the numerical results for the option pricing example using the values for,
$
T = 1/4, \ \ d = 13, \ \ S_0 = 100, \ \ r =  0.05, \ \ \sigma = 0.5, \ \ K = 200
$, same as used in the experiments of \code{cubBayesLattice\_g} \cite{RatHic19a}.
As mentioned before, this integrand has a kink caused by the $\max$ function, so, \code{cubBayesNet\_g} could be more efficient than \code{cubBayesLattice\_g}, as no periodization transform is required. This can be observed from the number of samples used for intgration to meet the same error threshold. For the error tolerance, $\varepsilon=10^{-3}$,  \code{cubBayesLattice\_g} used $n=2^{20}$ samples, whereas \code{cubBayesNet\_g} used $n=2^{17}$ samples.


\begin{figure}
\centering
\includegraphics[width=0.95\linewidth]{"figures/Sobol/Sobol_optPrice_guaranteed_time_MLE__d12_r1_2019-Sep-1"}
\caption[Sobol: Option pricing guaranteed: MLE]{Option pricing using the empirical Bayes stopping criterion. The hollow stars indicate the algorithm has not met the error threshold $\epsilon$ even with using maximum $n$.}
\label{fig:Sobol-optprice-guaranteed-MLE}
\end{figure}
\begin{figure}
\centering
\includegraphics[width=0.95\linewidth]{"figures/Sobol/Sobol_optPrice_guaranteed_time_full__d12_r1_2019-Sep-1"}
\caption[Sobol: Option pricing guaranteed: Full Bayes]{Option pricing using the full-Bayes stopping criterion. The hollow stars indicate the algorithm has not met the error threshold $\epsilon$ even with using maximum $n$.}
\label{fig:Sobol-optprice-guaranteed-FB}
\end{figure}
\begin{figure}
\centering
\includegraphics[width=0.95\linewidth]{"figures/Sobol/Sobol_optPrice_guaranteed_time_GCV__d12_r1_2019-Sep-1"}
\caption[Sobol: Option pricing guaranteed: GCV]{Option pricing using the GCV stopping criterion. The hollow stars indicate the algorithm has not met the error threshold $\epsilon$ even with using maximum $n$.}
\label{fig:Sobol-optprice-guaranteed-GCV}
\end{figure}




\subsection{Discussion}
\JRNote{Move this to end of chapter}

As shown in Figures \ref{fig:Sobol-mvn-guaranteed-MLE} to \ref{fig:Sobol-optprice-guaranteed-GCV}, both the algorithms computed the integral within user specified threshold most of the time except on a few occasions. This is especially the case with option pricing example due to the complexity and high dimension of the integrand. 
Also notice that the \code{cubBayesLattice\_g} algorithm finished within 10 seconds for Keister and multivariate Gaussian. Option pricing took closer to 70 seconds due to the complexity of the integrand.

Another noticeable aspect from the plots of \code{cubBayesLattice\_g} is how much the error bounds differ from the true error. For option pricing example, the error bound is not as conservative as it is for the multivariate Gaussian and Keister examples. A possible reason is that the latter integrands are significantly smoother than the covariance kernel.  This is a matter for further investigation.

% As shown in Figures \ref{fig:Sobol-mvn-guaranteed-MLE} to \ref{fig:Sobol-optprice-guaranteed-GCV}, the \code{cubBayesNet\_g} algorithm computes the integral within user specified threshold most of the times except on a few occasions. This is especially the case with option pricing example due to the complexity and high dimension of the integrand as mentioned in the previous example. 




Most noticeable aspect from the plots of \code{cubBayesNet\_g} is how closer the error bounds are to the true error. 
This shows that the \code{cubBayesNet\_g}'s estimation of expected error in the stopping criterion is very accurate. 
Similar to \code{cubBayesLattice\_g}, it missed meeting the given error threshold for the option pricing example, as marked by the hollow stars, for $\varepsilon=10^{-4}$. The algorithm reached max allowed number of samples, $n=2^{20}$ due to the complexity of the integrand.





\section{Conclusion and future work}
\label{sec:conclusion-future-work}
% Extended the fast Bayesian stopping criterion to use digital nets. Advantage is, integrand does not need to be periodic. Demonstrated using Matlab implementation.

\paragraph{TBD}


% \bibliographystyle{abbrvnat}
\bibliographystyle{spmpsci.bst}
\bibliography{FJHown23,FJH23}

\end{document}
